{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b1bd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Emma/Desktop/capstone/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tree import Tree\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import seaborn as sns\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4d0a2f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "d7eaa017",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"ch_copy/00/\"\n",
    "doc_id = '0037'\n",
    "\n",
    "#returns dictionary containing plain sentence, constituency parse and coreference-id-labeled tokens \n",
    "def get_sentence_profiles(doc_id):\n",
    "    profiles = {}\n",
    "    filename = \"ch_\" + doc_id + \".onf\"\n",
    "    fp = dir + filename\n",
    "    with open(fp, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    #enumerated = enumerate(lines) -- don't do this\n",
    "    plain_sent_idxs = [i for i, txt in enumerate(lines) if txt == (\"Plain sentence:\\n\")]\n",
    "    treebanked_idxs = [i for i, txt in enumerate(lines) if txt.find(\"Treebanked sentence:\\n\") > -1]\n",
    "    tree_idxs = [i for i, txt in enumerate(lines) if txt == (\"Tree:\\n\")]\n",
    "    leaves_idxs = [i for i, txt in enumerate(lines) if txt == (\"Leaves:\\n\")]\n",
    "\n",
    "    n_sents = len(plain_sent_idxs)\n",
    "\n",
    "    for i in range(n_sents):\n",
    "        profile = doc_id + \"_\" + str(i)\n",
    "        profiles[profile] = {}\n",
    "        profiles[profile][\"plain\"] = lines[(plain_sent_idxs[i] + 2):treebanked_idxs[i]][0].strip()\n",
    "        profiles[profile][\"plain\"] = profiles[profile][\"plain\"].replace(\"--\", \"\")\n",
    "        raw_tree = lines[(tree_idxs[i]+2):leaves_idxs[i]]\n",
    "        profiles[profile][\"tree\"] = process_tree(raw_tree)\n",
    "\n",
    "        if i < n_sents - 1:\n",
    "            profiles[profile][\"leafnotes\"] = lines[(leaves_idxs[i]+2):(plain_sent_idxs[i+1]-3)]\n",
    "        else:\n",
    "            profiles[profile][\"leafnotes\"] = lines[(leaves_idxs[i]+2):-3]\n",
    "\n",
    "        profiles[profile][\"leafnotes\"] = process_leaves(profiles[profile][\"leafnotes\"])\n",
    "        profiles[profile][\"leaves\"] = [leaf for leaf in Tree.fromstring(profiles[profile][\"tree\"]).leaves() if '*' not in leaf]\n",
    "\n",
    "        #i += 1\n",
    "    return profiles\n",
    "\n",
    "def process_leaves(leafnotes):\n",
    "    leaves_dict = {}\n",
    "    i = 0\n",
    "    for line in leafnotes:\n",
    "        line = line.strip()\n",
    "        if len(line) > 0:\n",
    "            line = line.split()\n",
    "            if line[0].isdigit():\n",
    "                if line[1] != \"--\":\n",
    "                    leaves_dict[i] = {\"token\":\"\", \"info\": {}}\n",
    "                    leaves_dict[i][\"token\"] = line[1]\n",
    "                    i += 1\n",
    "            elif i-1 in leaves_dict:\n",
    "                leaves_dict[i-1][\"info\"][line[0]] = line[1:]\n",
    "       \n",
    "    return leaves_dict\n",
    "\n",
    "def process_tree(raw_tree):\n",
    "    tree = \"\"\n",
    "    for line in raw_tree:\n",
    "        tree += line.strip()\n",
    "    return tree\n",
    "\n",
    "def get_doc_profile(doc_id):\n",
    "    profile = {}\n",
    "    sentence_profiles = get_sentence_profiles(doc_id)\n",
    "    profile[\"plain\"] = \"\"\n",
    "    for id in sentence_profiles:\n",
    "        profile[\"plain\"] += sentence_profiles[id][\"plain\"]\n",
    "    profile[\"trees\"] = [sentence_profiles[profile][\"tree\"] for profile in sentence_profiles]\n",
    "\n",
    "    #creates a list of dictionaries containing leafnotes for each sentence\n",
    "    profile[\"leaves_per_tree\"] = []\n",
    "    for id in sentence_profiles:\n",
    "        profile[\"leaves_per_tree\"].append(deepcopy(sentence_profiles[id][\"leafnotes\"])) #must deepcopy or dictionaries will merge\n",
    "\n",
    "    #merged leaves\n",
    "    profile[\"leafnotes\"] = sentence_profiles[doc_id + \"_0\"][\"leafnotes\"]\n",
    "    sentence_profiles_tail = dict(list(sentence_profiles.items())[1:])\n",
    "    i = list(profile[\"leafnotes\"].keys())[-1]\n",
    "    for id in sentence_profiles_tail:\n",
    "        for leaf in sentence_profiles[id]['leafnotes']:\n",
    "            i += 1\n",
    "            profile[\"leafnotes\"][i] = sentence_profiles_tail[id]['leafnotes'][leaf]\n",
    "\n",
    "    #words\n",
    "    profile[\"leaves\"] = []\n",
    "    for tree in profile[\"trees\"]:\n",
    "        leaves = Tree.fromstring(tree).leaves()\n",
    "        profile[\"leaves\"] += ([leaf for leaf in leaves if '*' not in leaf])\n",
    "\n",
    "    return profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "986e8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_profiles = get_sentence_profiles(doc_id)\n",
    "doc_profile = get_doc_profile(doc_id)\n",
    "sample = s_profiles['0037_0']#['plain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "448cfe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['就', '觉', '挺', '不', '舒服', '的', ',', '本来', '就是', '信心', '挺', '足', '的', '.', '啊', '.', '啊', '.', '后来', '我', '说', '小穗', '啊', ',', '你', '既然', '你', '走', '这', '一', '步', '了', '好', ',', '你', '找', '人', '算了', '这个', '哈', '.', '我', '说', '这样', '吧', ',', '我', '说', '我', '认识', '一', '个', '人', '就是', '起', '那个', '八卦', '呀', '.', '嗯哼', '.', '嗯哈', '.', '起', '的', '挺', '准', '的', ',', '他', '就是说', ',', '就说', '某', '一', '件', '事儿', ',', '在', '看', '人', '的', '大运', '哈', '.', '反正', '我', '我', '知道', '的', '哈', ',', '他', '说', '我', '和', '我', '认识', '这些', '人', '哈', ',', '真', '说', '得', '挺', '准', '的', '.', '呃哼', '.', '呃哈', '.', '呃哈', '.', '哦', ',', '是', '吗', '?', '我', '说', '这样', '吧', ',', '反正', '你', '已经', ',', '已经', '呃', '就是', '算', '过', '了', '哈', '.', '我', '说', '找', '你', '帮', '她', '看', '一', '看', '吧', '.', '晚上', '我', '就', '去', '找', '人', '去', '了', '.', '嗯哼', '.', '嗯哈', '.', '好好', '的', '给', '小穗', '算', '哈', ',', '让', '小穗', '哈', '立秋', '以后', '.', '哎', '我', '记得', '你', '那', '年', '走', ',', '是', '不', '是', '就是', '立秋', '以后', '?', '啊呵', '.', '我', '是', '八月', '四号', '到', '的', '啊', ',', '我', '就', '是', '明天', '到', '的', ',', '到', '的', '美国', '.', '当然', '那时候', '已经', '立秋', '了', '吧', '?', '从', '农历', '上', '讲', '.', '我', '不', '知道', '哎', '.', '嗯', ',', '我', '怎么', '当时', '有', '一', '个', '印', '-', ',', '我', '当时', '有', '个', '印象', ',', '你', '好象', '也', '是', '立秋', '啊', ',', '有', '一', '个', '立秋', '什么', '说到', '.', '就是', '跟', '她', '说', '.', '嗯哼', '.', '后来', '呢', '就', ',', '我', '就', '跟', '他', '说', '了', '这', '件', '事儿', ',', '把', '小', '-', '小穗', '的', '生辰八字', '给', '他', '了', '哈', '.', '就', '让', '他', '陈', '这', '一', '件', '事儿', ',', '就', '说', '近', '要', '去', '那个', '什么', ',', '问', '行', '不', '行', '.', '他', '不', '让', '她', '动', ',', '他', '让', '她', '八月', '八号', '立秋', '以后', '.', '嗯哈', '.', '嗯哈', '.', '嗯哈', '.', '呃哈', '.', '八月', '八号', '立秋', '.', '啊', ',', '他', '说', '哈', ',', '她', '立秋', '以后', '.', '我', '说', '能', '不', '能', '那个', '什么', ',', '他', '说', '小穗', '排', '大运', '排', '到', '三十二', '岁', '以前', '哈', ',', '有', '这个', '运', '的', '.', '嗯哼', '.', '哎哟', ',', '我', '就', '赶紧', '第二', '天', '回来', '告诉', '小穗', ',', '我', '说', '小穗', ',', '我', '说', '人家', '说', '你', '肯定', '那个', '能', '那个', '什么', '.', '嗯哈', '呵呵', '.', '我', '说', '但是', '呢', ',', '是', '有', '一', '个', '时间', '那个', '限制', '.', '我', '说', '整好', '差', '一', '星', '-', '.', '她', '本来', '想', '这个', '星期一', '嘛', ',', '去', '嘛', '.', '下', '星期', '能', '挪', '啵', '下', '-', ',', '下', '个', '星期一', '啦', '.', '下', '星期一', '正好', '那', '天', '立秋', '.', '是', '啊', '.', '嗯哈', '.', '那', '八月', '八号', '就', '是', '下', '个', '星期一', '.', '啊', '.', '呀', '.', '啊', '那', ',', '我', '说', '你', '这样', '吧', '.', '我', '说', '你', '拖', '一', '个', '星期', '吧', '.', '我', '说', '反正', '呀', ',', '嗯哈', '.', '锺监虹', '他', '妈', '给', '你', '算', '了', '以后', '你', '心情', '也', '不', '好', '.', '你', '去', '了', '呃', '.', '嗯呵', '.', '嗯呵', '.', '呃呵', ',', '哦', '.', '哎哟', ',', '看', '了', '锺监虹', '就', '约', '好', '了', ',', '锺监虹', '直接', '.', '啊', ',', '怎么', '现在', '小穗', ',', '信', '这个', '这么', '信', '啊', '现在', '.', '啊', '?', '不', '是', '啊', ',', '她', '要是', '其实', '不', '应该', '算', '的', '这', '种', '事儿', '啊', '.', '但是', '呢', '就是', '锺监虹', '这个', '人', ',', '就', '找', '他', '妈', '不', '知道', '是', '找', '谁', '给', '算', '了', ',', '算', '的', '小慧', ',', '小穗', '情绪', '就', '挺', '不', '高', '的', ',', '你', '知道', '.', '我', '觉得', '这', '要', '是', '情绪', '吧', ',', '肯定', '会', '对', '这', '事儿', '有', '影响', '的', '.', '你', '说', '你', '回答', '问题', '啊', '什么', '自信心', '.', '啊', '.', '动', '了', '一', '个', ',', '嗯哼', '.', '哦', ',', '呀', '.', '的', '.', '对', '.', '肯定', '要', '受', '影响', '的', ',', '干脆', '我', '说', '你', '既然', '这样', '的话', ',', '我', '说', '就', '赶紧', '找', '个', '人', '吧', '.', '对', '啊', ',', '唉', ',', '我', '我', ',', '啊哈', '.', '我', '说', '不见得', '那个', '人', '就', '算', '得', '很', '准', ',', '我', '心想', '要是', '有', '什么', ',', '就是', '算', '得', '不', '好', '了', ',', '我', '回头', '也', '跟', '小穗', '说', '挺', '好', '的', '.', '啊哈', '.', '呀', ',', ',', '对', ',', '对', '啊', ',', '对', '啊', '.', '嗯', '能', '鼓励', '她', '啦', ',', '你', '要是', '让', '她', '情', '-', '这么', '好', ',', '她', '肯定', '不行', '了', '.', '所以', '说', '那个', '锺监虹', '当然', '已经', '联系', '不', '上', '啦', '.', '锺监虹', '直接', '从', '沈', '-呃', ',', '从', '上海', '到', '沈阳', '去', '了', '.', '直接', '坐', '那', '火车', '走', '了', ',', '就', '没', '坐', '船', '过来', '.', '对', '啊', ',', '对', '啊', '.', '嗯哈', '.', '啊', '.', '唉', '我', '跟', '你', '说', '啊', ',', '嫂', '啊', '.', '对', ',', '唉', '.', '你', '听', '我', '说', '啊', ',', '一', '个', '那个', '什么', ',', '因', '-', '她', '那个', '学校', '里头', '写', '的', '吧', ',', '是', '八月', '二十八号', ',', '要求', '的话', '就', '报', '-', ',', '她', '到', '学校', '的', '时间', '.', '所以', '这个', '时间', '她', '要', '注意', '.', '不', '能', '去', '的', '太', '晚', ',', '所以', '她', '踢推', '到', '八号', '还', '可以', ',', '但是', '我', '劝', '她', '不能', '再', '呗', '.', '嗯', '.', '哦', '.', '嗯', '.', '她', '现在', '就', '定', '了', '八号', '了', '.', '对', ',', '如果', '要是', '再', '推', '的话', ',', '那', '就', '麻烦', '了', '.', '比如说', '领事', '就', '会', ',', '觉得', '就是说', ',', '你', '没有', '足够', '的', '时间', '去', '买', '飞机票', '.', '没有', '足够', '的', '时间', '到', '学校', ',', '她', '估计', '你', '没有', '这些', '足够', '的', '时间', '.', '所以', '他', '可能', '会', '因为', '这些', '他', '就', '可能', '会', ',', '会', '不', '给', '你', '签', '.', '去', '签', '是', '吧', '.', '对', ',', '所以', '说', ',', '就是说', ',', '她', '推', '可以', ',', '但', '八月', '八号', '我', '觉', '还', '可以', ',', '但是', '再', '推', '的话', ',', '我', '觉得', '就是', '拖', '得', '时间', '太', '长', '了', '.', '喂', '?', '那', '现在', ',', '后来', '她', '跟', '锺监虹', '回来', '就', '定', '了', ',', '下', '周一', '八月', '八号', '去', '签', '.', '是', '啊', ',', 'ｏｋａｙ', '，', '那', '你', '还', '有', '一', '个', '就是说', '呢', ',', '还', '有', '一', '还', '有', '一', '个', '就是说', '怎么', '提醒', '小石', '要', '特别', '注意', '的', '就', '是', '什么', '呢', '.', '又', '美国人', ',', '美国人', '这边', '他', '特别', '讲究', '这个', ',', '你', '要', '有', '这', '种', 'ｃｏｎｆｉｄｅｎｃｅ', '．', '啊', ',', '我', '说', '呢', ',', '呃', '.', '呃', '.', '那', ',', '你', '你', '没有', 'ｃｏｎｆｉｄｅｎｃｅ', '他', '说话', '一', '这么', '躲躲', ',', '躲躲闪闪', '的', '你', '知道', '.', '反正', '没事', '的', '事情', '人家', '会', '怀疑', ',', '以为', '她', '是', '在', '说假', '.', '哦', '.', '呃', '.', '一定', '要', '有', 'ｃｏｎｆｉｄｅｎｃｅ', '，', '这', '一', '点', '很', '重要', '.', '就是说', '你', '尤其', '象', ',', '比如说', '象', '我们', '找', '工作', ',', '什么的', '是', '去', '这', '种', '见', '人', 'ｉｎｔｅｒｖｉｅｗ', '什么', '这', '种', '.', '喔', '.', '哦', '.', '嗯', '.', '都', '是', '最', '重要', '的', '一', '点', '就是说', '你', '一定', '要', ',', '你', '不', '能', '自己', '不', '缩头缩脑', '的', '你', '知道', '啊', '.', '你', '本来', '能', '做', '的', '事情', '你', '不', '敢', ',', '不', '敢', '全', '说', '你', '能', '做', '.', '或者说', '有点', '软', ',', '你', '一', '说', '软话', '.', '嗯', '.', '嗯', '.', '本来', '你', '实际上', '都', '可以', '做', '的', '事情', ',', '你', '都', '以前', '做', '过', '的', '事情', '.', '但是', '你', '一', '说', '软话', ',', '马上', '人家', '对方', '就', '以为', ',', '你', '是', '不行', '.', '呐', '所以', '提醒', '她', '这', '一', '点', '特别', '要', '注意', '.', '呃', '.', '就是', '那个', '好象', ',', '哦', '自信心', '要', '强', '一点', '.', '呀', '.', '就', '一定', '提醒', '她', '这', '讲话', '的', '时候', ',', '一定', '要', '注意', '她', '这个', '讲话', '的', '时候', ',', '不', '能', '那个', '缩头缩脑', '不', '能', '那个', '什么', ',', '你', '知道', '.', '就是', '特别', '要', '注意', '她', '要', ',', '要', '有', '这个', ',', '这个', '讲话', '时候', '要', '注意', '.', '嗯', '.', '嗯', '.', '嗯', '.', '哦', '行', ',', '哦', '行', ',', '一会儿', '过去', '跟', '她', '说', '一', '说', '去', '嗯', '.', '呀', '.', '还', '好', '那', '还', '反正', ',', '那', '我', '就', '等到', '下', '礼拜', ',', '也许', '下', '个', '礼拜', ',', '也许', '我', '还', '能', '有', '一', '个', '这样', '机会', '吧', '.', '呀', ',', '好', '.', '呀', '也许', '我', '可能', '还', '能', '打', '.', '是', '吗', ',', '呃', '对', '啊', ',', '它', '十五', '分钟', ',', '到', '十五', '分钟', '它', '会', '自己', '断掉', '.', '自己', '断掉', '哈', ',', '哎', '.', '对', '当然', '这', '十五', '分', '-', ',', '这', '十五', '分钟', '是', 'ｆｒｅｅ', '的', ',', '呀', ',', '你', '说', '吧', '.', '哦', ',', '那', '什么', '啊', '.', '嗯哼', '.', '那个', '什么', '你', '能', '定', '下来', ',', '下次', '什么', '来', '电话', '吗', '?', '我', ',', '下', '呀', '她', ',', '对', ',', '她', '因为', '是', '礼拜一', '是', '啊', '.', '她', '八月', '八', ',', '她', '八月', '八号', ',', '那么', '我', '就', '礼拜', ',', '在', '八号', '以前', '.', '对', '呀', '.', '但是', '她', ',', '我', '跟', '你', '讲', '哈', ',', '她', '星期天', '就', '得', '走', ',', '七号', '就', '得', '走', '.', '啊哈', '.', '嗯', ',', '她', '七号', '就', '要', '呃', '到', '沈阳', '了', '.', '啊', '.', 'Ｏｋａｙ', '，', '如果', '我', '要是', ',', '如果', '我', '要', '能', '给', '你', '在', '这个', ',', '啊', '她', '七号', '到', '沈阳', '是', '吧', '?', '到', '沈阳', '嗯', ',', '那', '肯定', '要', '七号', '走', ',', '星期天', '走', '的', '呃', '.', '对', '那', '就是', '她', '可能', '礼拜天', '走', '.', '嗯哈', '.', '那', '有', '可能', '的话', ',', '我', '会', '在', '礼拜六', ',', '要', '打', '的话', '我', '会', '在', '礼拜六', '早上', '打', ',', '就', '是', '你们', '礼拜六', '的', '晚上', '.', '嗯', '.', '因为', '她', '礼拜六', '还', '不', '可能', '走', '吧', '?', '她', '礼拜六', '会', '走', '吗', '?', '啊', ',', '礼拜六', '不', '会', '走', '的', ',', '不', '会', '的', '.', '嗯', ',', '嗯', '.', '礼拜六', '还', '不', '会', '走', '啊', ',', '那', 'ｏｋａｙ', '，', '那', '如果', '要', '这么', '的话', ',', '我', '会', '在', '礼拜六', ',', '我', '的', '上午', '打', ',', '也', '就是说', '你们', '的', '晚上', ',', '礼拜六', '的', '晚上', '.', '晚上', ',', '嗯', ',', '好', '.', '呀', ',', '那么', ',', '我', '打', '的话', ',', '一般', '也', '都', '是', ',', '呃', ',', '我', '要', '打', '的话', '.', '礼拜六', '我', '要', ',', '如果', '要', '打', '的话', ',', '就是', '在', '你们', '晚上', '大概', '八点半', '之前', '打', '.', '嗯', '.', '哦', '.', '最', '呃', ',', '最', '起码', '是', '我', '想想', '啊', ',', '七点', '我', '这边', '七点', ',', '你们', '是', '九点钟', '之前', '.', '嗯', '.', '九点钟', '之前', ',', '啊', '.', '对', ',', '我', '估计', '会', '在', '你们', '晚', '-', ',', '礼拜六', '晚上', '九点钟', '之前', '打', '.', '嗯', ',', '好', '.', '对', ',', '如', '-', '我', '可能', ',', '可能', '还', '会', '有', '一', '个', 'ｆｒｅｅ', '的', '这样', '的', '机会', ',', '也许', '.', '嗯', ',', '好', '.', '那', '我', '可能', '打', ',', '就', '是', '在', '那个', '时候', '打', '.', '我', '想', '最好', '她', ',', '她', '去', '之前', ',', '你', '能', '跟', '她', '再', '通', '一次', '话', '.', '给', '她', '打打', '气儿', '啊', '.', '对', ',', '对', ',', '我', '想', '呀', '.', '对', ',', '我', '想', '那么样', '嘛', ',', '那', '就', '差不多', '我', '礼拜六', '的', '时候', ',', '我', '再', '打电话', '跟', '她', '说', '一', '说', '.', '呃', ',', '因为', '什么', '我', '觉得', '她', '那', '天', '接', '了', '电话', '以后', ',', '好象', '情绪', '不', '是', '很', '高', '的', '.', '就', '锺监虹', '跟', '她', '说', '完', '之后', '.', '是', '啊', '.', '我', '觉得', '真', '讨厌', ',', '我', '说', '你', '那', '算命', '干啥', '.', '那', '那', '小穗', '她', '本来', '哪', '就', '是', ',', '就', '是', '好象', '觉得', '聘', '一次', '的话', ',', '心里', '就', '不', '是', '很', '有', '把握', '的', '.', '呃', '.', '嗯哼', ',', '呀', '.', '她', '就是', '心', '-', ',', '啊', '然后', '再', '一', '算', '你', '又', '不', '好', ',', '你', '也', '别', '跟', '她', '说', '呀', '.', '早', '还', '告诉', '她', '了', ',', '就', '惹', '了', '她', ',', '她', '特别', ',', '我', '看', '自信心', '真', '是', ',', '好象', '差', '了', '好多', '.', '啊哈', '.', '是', '啊', ',', '嗯', ',', '但是', '我', '觉得', '我', '跟', '她', '说', '完', '之后', ',', '她', '也', '有点儿', '信', '了', '.', '人', '到', '这', '时候', '哈', ',', '就', '是', '什么', ',', '就', '别人', '说', '什么', '可能', '事', ',', '她', '都', '有点儿', ',', '哎', ',', '哎哟', ',', '我', '觉得', '有时候', '没', '办', '-', '.', '是', '啊', '哦', '我', '跟', '她', ',', '我', '在', '美国', '尤其', '比如', '找', '工作', ',', '前', '一', '段', '什么', '时间', '弄', '啊', '弄', ',', '后来', '也', '是', '就', '也', '觉得', '这样', '的', '事', ',', '有时候', '也', '就', '会', '有点', '信', '了', '.', '是', '啊', ',', '反正', '我', '觉得', '现在', '吧', ',', '就是说', '可能', '是', '科学', '要是', '越来越', '发展', ',', '这些', '东西', '象', '跟着', '抬头', '了', '.', '因为', '人', '应该', '有', '信仰', ',', '现在', '谁', '也', '不', '信', '那', '什么', '了', '.', '呀', ',', '呀', '.', '呀', '.', 'ｒｉｇｈｔ', '．', '所以说', '都', '找', '一', '个', '地方', ',', '什么', '东西', '信', '一', '信', '吧', '.', '不', ',', '我', ',', '我', '觉得', '很', '有', '可能', '哎', ',', '因为', '我', ',', '我', '当初', '就', '是', '.', '嗯', '.', '我', '找', '工作', '上面', '的', '也', '是', '也', '是', '说', ',', '就', '说', '你', '到', '最后', '叫', '背水一战', '的', '时候', ',', '肯定', '能', '成功', '.', '嗯', '.', '然后', ',', '对', ',', '然后', ',', '后', '然后', '就', '没事', '啊', ',', '果真', '中', '啊', '.', '是', '不', '-', ',', '所以说', '那个', '什么', ',', '我们', '也', '那个', '他们', '也', '是', '.', '他们', '说', '呀', ',', '说', '已经', '说', '你', '要是', '不', '打算', '再', '给', '她', '办', '了', ',', '不', '是', '吗', '.', '那', '她', '今年', '肯定', '中', '.', '呃', '.', '啊', '.', '呃', '.', '因为', '我', '还', '有', '几', '个', '朋友', '也', '是', ',', '也', '都', '是', ',', '就是说', '临到', '最后', '说', ',', '这', '次', '再', '去', '签', '一次', '.', '不', '签', ',', '签', '不', '出来', '就', '不', '去', '啦', '.', '就', '就', '不', '弄', '了', ',', '你', '知道', '.', '嗯', '.', '哦', '最后', '成功', '了', '.', '啊', '.', '啊', ',', '都', '成功', '了', ',', '呃', ',', '但是', '我', '给', '你', '说', '哈', ',', '她们', '原来', '给', '小穗', '算', '哈', ',', '她', '说', '有', '这个', '命', '的', '.', '呃哼', '.', '呃', ',', '是', '啊', '.', '啊', ',', '他', '小穗儿', '那个', ',', '最好', '的', '时候', '还', '没', '到', '呢', ',', '三十二', '岁', '呢', '.', '我', '说', '穗儿', ',', '你', '肯定', '是', '出去', '赚钱', '啦', '.', '我', '说', '那', '国内', '你', '可能', '赚', '不', '了', '大钱', '.', '啊', '.', '啊', 'ｏｋａｙ', '吧', '好', '了', ',', '小穗儿', '我', '看', '还', '能', '好', '一点', '哈', '不', ',', '就', '暂时性', '的', '呢', '能', '好', '一点', ',', '我', '看', '.', '啊哈', '.', '对', '啊', '.', '啊', '没', '办法', ',', '不', '都', '是', '要', '信', '一点儿', '.', '嗯', '.', '哎', '怎么样', ',', '你', '那边', '气候', '现在', '?', '哦', '加州', '这边', '气候', '好', '的', '很', '啊', '.', '晚上', '的话', '跟', '大连', '一样', ',', '白天', '的话', '比', '大连', '可能', '还要', ',', '还要', '凉快', '.', '对', '吗', '.', '哎哟', '.', '就', '我', '住', '的', '这边', '.', '哎哟', '大连', '今天', '特别', '特别', '反常', '.', '哦', ',', '是', '吗', '?', '啊', ',', '外面', '都', '三十五', '六', '度', '啊', '.', '哦', ',', '那', '我们', '这', '没', '-', '没有', ',', '到', '不', '了', '那么', '多', '.', '就', '白天', ',', '晚上', '有时候', '热', '得', '哈', ',', '人', '都', '睡', '不', '着', '觉', '.', '是', '吗', ',', '哦', '那', '太', '高', '.', '啊', '就是', ',', '现在', '就是', '基本上', '哈', ',', '我', '看', '今年', '要是', '空调', '进', '不', '了', '家', '的话', '哈', '.', '明年', '后年', '势必', '进入', '家庭', ',', '是', '一', '个', '趋势', '.', '哈尔滨', '你', '想想', ',', '白天', '都', '三十八', '度', '了', '.', '嗯哈', '.', '哦', ',', '是', '吗', '?', '哟', ',', '那么', '高', '啊', ',', '三十八', '度', '啊', '.', '哎', '现在', '是', '南方', '凉快', ',', '北方', '热', '.', '怪', '了', '.', '特别', '特别', '奇怪', ',', '就是', '南方', '因为', '它', '雨水', '成灾', '吗', '.', '就是说', '好象', '气候', '还', '凉快', '一点', '.', '象', '深圳', '吧', ',', '我', '去年', '在', '的', '时候', '都', '是', '三十四', '五', '度', '.', '今年', '到', '二十七', '八', '度', '了', '.', '啊', '.', '啊', '是', '啊', ',', '那', '我', '那', '我', '这边儿', ',', '嗯', '大连', ',', '嗯', '.', '我', '这边儿', '现在', '你', '要', '晚上', '的话', ',', '你', '还', '必须', '穿', '外套', '才', '行', '.', '否则', '冻', '的话', ',', '冷', '的话', '.', '+', '旧金山', '+', '是', '.', '哎呀', ',', '你', '知道', '我', '现在', '住', '这', '地方', ',', '不', '是', '两', '家', '住', '着', '吗', '.', '啊哈', '.', '是', '两', '家', '合', '住', '的', '.', '然后', '呢', ',', '就是说', '公用', '的', '厨房', ',', '公用', '的', '那个', '洗手间', '.', '啊哈', '.', '啊', ',', '不', '方便', '.']\n",
      "{0: {'token': '*pro*', 'info': {'coref:': ['IDENT', '4', '0-0', '*pro*']}}, 1: {'token': '就', 'info': {}}, 2: {'token': '觉', 'info': {'prop:': ['觉.01'], 'v': ['*', '->', '2:0,', '觉'], 'ARG0': ['*', '->', '0:0,', '*pro*'], 'ARGM-DIS': ['*', '->', '1:1,', '就'], 'ARG1': ['*', '->', '3:3,', '*pro*', '挺', '不', '舒服', '的']}}, 3: {'token': '*pro*', 'info': {'coref:': ['IDENT', '5', '3-3', '*pro*']}}, 4: {'token': '挺', 'info': {}}, 5: {'token': '不', 'info': {}}, 6: {'token': '舒服', 'info': {'prop:': ['舒服.02'], 'v': ['*', '->', '6:0,', '舒服'], 'ARG0': ['*', '->', '3:0,', '*pro*'], 'ARGM-ADV': ['*', '->', '5:1,', '不']}}, 7: {'token': '的', 'info': {}}, 8: {'token': ',', 'info': {}}, 9: {'token': '本来', 'info': {}}, 10: {'token': '就是', 'info': {}}, 11: {'token': '信心', 'info': {}}, 12: {'token': '挺', 'info': {}}, 13: {'token': '足', 'info': {'prop:': ['足.01'], 'v': ['*', '->', '13:0,', '足'], 'ARGM-ADV': ['*', '->', '12:1,', '挺'], 'ARGM-DIS': ['*', '->', '10:1,', '就是'], 'ARG0': ['*', '->', '11:1,', '信心']}}, 14: {'token': '的', 'info': {}}, 15: {'token': '.', 'info': {}}}\n",
      "['就', '觉', '挺', '不', '舒服', '的', ',', '本来', '就是', '信心', '挺', '足', '的', '.']\n"
     ]
    }
   ],
   "source": [
    "print(doc_profile['leaves'])\n",
    "print(s_profiles[doc_id+\"_0\"][\"leafnotes\"])\n",
    "print(s_profiles[doc_id+\"_0\"][\"leaves\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "\n",
    "##word-level\n",
    "def get_words(sentence_profile):\n",
    "    leaves = sentence_profile[\"leafnotes\"]\n",
    "    #words = [(idx, leaves[i][\"token\"]) for idx, i in enumerate(leaves)]\n",
    "    words = [leaves[i][\"token\"] for i in leaves]\n",
    "    return words\n",
    "\n",
    "##character-level\n",
    "def get_characters(sentence_profile):\n",
    "    #characters = [(idx, ch) for idx, ch in enumerate(sentence_profile[\"plain\"])]\n",
    "    characters = [ch for ch in sentence_profile[\"plain\"]]\n",
    "    return characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d59b0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model and tokenizer\n",
    "model_name = \"TsinghuaAI/CPM-Generate\"#\"ckiplab/gpt2-base-chinese\" #\"hfl/chinese-bert-wwm\" #\"bert-base-chinese\"\n",
    "#tokenizer = \"hfl/chinese-bert-wwm\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#align annotated leaves with model inputs\n",
    "'''def filter_inputs(inputs, tokens, leaf_labels): #manipulate output matrix instead?\n",
    "    inputs['input_ids'] = inputs['input_ids'].numpy()\n",
    "    inputs[]\n",
    "    for i in range(len(tokens)):\n",
    "        if i >= len(leaf_labels):\n",
    "            break\n",
    "        if tokens[i][:1] != leaf_labels[i]:\n",
    "\n",
    "            del inputs[i]\n",
    "    return inputs'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8172305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_attn(attn_matrix, leaves, tokens):\n",
    "    #attn_matrix: 2-D numpy array\n",
    "    print(\"tokens:\",tokens)\n",
    "    print(\"leaves:\",leaves)\n",
    "    filter = []\n",
    "    for i in range(len(leaves)):\n",
    "        if i >= len(tokens):\n",
    "            break\n",
    "        if tokens[i] == '▁': #▁ is not an underscore (_)\n",
    "            #print(tokens[i])\n",
    "            del tokens[i]\n",
    "        print(tokens[i], leaves[i])\n",
    "        if tokens[i].replace('▁', '') != leaves[i]:\n",
    "            filter.append(i)\n",
    "    attn_matrix = np.delete(attn_matrix, filter, axis = 0)\n",
    "    attn_matrix = np.delete(attn_matrix, filter, axis = 1)\n",
    "    return attn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "f77405ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = 0\n",
    "layers = 0\n",
    "\n",
    "#returns attention weights for specified head, layer\n",
    "def attention_map(model, profile, heads, layers):\n",
    "    inputs = tokenizer(profile[\"plain\"], return_tensors=\"pt\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    outputs = model(**inputs)\n",
    "    print(torch.stack(outputs.attentions).shape)\n",
    "    attention_matrix = torch.stack(outputs.attentions)[layers, :, heads, :, :].squeeze(0).detach().numpy()\n",
    "    print(attention_matrix.shape)\n",
    "    # = attention[layers.unsqueeze(0), :, heads, :, :]\n",
    "    #print(selection.shape)\n",
    "    #aggregate = attention.sum(dim = 0).detach().numpy()\n",
    "    #print(aggregate.shape)\n",
    "    \n",
    "    \n",
    "    print(len(tokens), tokens)\n",
    "    #punct_idxs = [idx for idx, token in enumerate(tokens) if token in [\",\", \"[CLS]\", \"[SEP]\"]]\n",
    "    #tokens = [token for i, token in enumerate(tokens) if i not in punct_idxs]\n",
    "    #attention_matrix = np.delete(attention_matrix, punct_idxs, axis = 1)\n",
    "    #attention_matrix = np.delete(attention_matrix, punct_idxs, axis = 0)\n",
    "    attention_matrix = filter_attn(attention_matrix, profile[\"leaves\"], tokens)\n",
    "    return attention_matrix #return aggregate.detach().numpy()\n",
    "    #tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d6330eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the index of some verb in the predicate corresponding to the null subject\n",
    "def get_verb_idx(tree, nsubj_idx):\n",
    "    t_positions = tree.treepositions(order = 'preorder')\n",
    "    leaf_positions = [tree.leaf_treeposition(idx) for idx in range(len(tree.leaves()))]\n",
    "    nsubj_leaf_position = tree.leaf_treeposition(nsubj_idx)\n",
    "    #print(\"get_verb_idx nsubj_idx:\", nsubj_idx)\n",
    "    IP_idx = t_positions.index(nsubj_leaf_position) - 1\n",
    "    IP_pos = t_positions[IP_idx]\n",
    "    IP_tree = tree[IP_pos]\n",
    "\n",
    "    while (type(IP_tree) == type(\"s\") or IP_tree.label() not in ['INC', 'IP']):\n",
    "        IP_idx -= 1\n",
    "        IP_pos = t_positions[IP_idx]\n",
    "        IP_tree = tree[IP_pos]\n",
    "\n",
    "    '''if type(IP_tree) == type(\"string\"):\n",
    "        print(tree.leaves()[nsubj_idx])\n",
    "        print(IP_tree)\n",
    "        print(type(IP_tree), tree)'''\n",
    "        \n",
    "    IPt_positions = IP_tree.treepositions(order='preorder')\n",
    "    mv_idx = None\n",
    "    for i in range(len(IPt_positions)):\n",
    "        position = IPt_positions[i]\n",
    "        if type(IP_tree[position]) == str and ('V' in IP_tree[IPt_positions[i-1]].label() or 'NT' in IP_tree[IPt_positions[i-1]].label()): #to-do: optimize\n",
    "            #print(IP_tree[IPt_positions[i-1]])\n",
    "            mv_idx = leaf_positions.index(t_positions[IP_idx + i])\n",
    "            #print(mv_idx)\n",
    "            return mv_idx\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def reindex_attn_matrix(attn_matrix, leaves):\n",
    "    for leaf in leaves'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "110369bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for layers adhering to Cf rankings\n",
    "#...examine effect of context_size on first-mention effect\n",
    "#returns ordered indices of candidate antecedents\n",
    "'''def mention_sequence(tree):\n",
    "    lleaves = tree.pos()\n",
    "    return [(leaf[0], i) for i, leaf in enumerate(lleaves) if \"N\" in leaf[1] and \"*-\" not in leaf[0]]'''\n",
    "def mention_idxs(trees):\n",
    "    joined_trees_tags = []\n",
    "    for tree in trees:\n",
    "        joined_trees_tags += [leaf for leaf in tree.pos()]\n",
    "    return [idx for idx, leaf in enumerate(joined_trees_tags) if \"N\" in leaf[1] and \"*-\" not in leaf[0]]\n",
    "'''def mention_set(string):\n",
    "    return set(mention_sequence(string)) #ensure that order is preserved'''\n",
    "\n",
    "#Hypothesis 1：linear order (looking for first-mention and recency effect) #filter out effect of confound---generate new set of sentences, \n",
    "##check if effect disappears w/ bidirectional model\n",
    "def coref_map(mention_idxs, leafnotes): #convert mention indices to coreference ids\n",
    "    coref_ids = []\n",
    "    for idx in mention_idxs:\n",
    "        #print(idx, leafnotes[idx])\n",
    "        if \"coref:\" in leafnotes[idx][\"info\"]:\n",
    "            #print(leafnotes[idx][\"info\"][\"coref:\"])\n",
    "            coref_ids.append(int(leafnotes[idx][\"info\"][\"coref:\"][1]))\n",
    "        else:\n",
    "            coref_ids.append(-1)\n",
    "    return coref_ids\n",
    "\n",
    "'''def hyp_1_pred(profile):\n",
    "     #todo\n",
    "     return None'''\n",
    "                \n",
    "def compare_rankings(hyp_rankings, attn_rankings):\n",
    "     #diffs = [abs(attn_rankings.index(attn_idx) - hyp_rankings.index(attn_idx)) for attn_idx in attn_rankings]\n",
    "     #print(hyp_rankings)\n",
    "     #print(attn_rankings)\n",
    "     total_diff = sum([hyp_rankings[i] != attn_rankings[i] for i in range(len(attn_rankings))])\n",
    "     return total_diff\n",
    "\n",
    "#returns list of candidates ranked by sums of attentions corresponding to each candidate\n",
    "def aggregate_attentions(candidate_attns, coref_ids):\n",
    "    coref_set = set(coref_ids)\n",
    "    aggregates_per_mention = {id:0 for id in coref_set}\n",
    "    for i in range(len(candidate_attns)):\n",
    "        aggregates_per_mention[coref_ids[i]] += candidate_attns[i][0]\n",
    "    new_rankings = sorted([(id, aggregates_per_mention[id]) for id in coref_ids], key = lambda tup: tup[1])\n",
    "    new_rankings = [tup[0] for tup in new_rankings]\n",
    "    return new_rankings\n",
    "\n",
    "#to-do: aggregate attentions for each coreference id, then rank and compare with order of coreference ids as predicted by hypothesis -- done?\n",
    "\n",
    "def test_hyp1_doc(profile, attention_maps, context_len=512):\n",
    "\n",
    "    comparisons = {} #dictionary mapping attention matrix to ranking comparisons\n",
    "    trees = [Tree.fromstring(tree) for tree in profile[\"trees\"]]\n",
    "    mentions = mention_idxs(trees)\n",
    "\n",
    "    nsubj_counter = 0 #keep track of doc-level nsubj positions\n",
    "\n",
    "    for i in range(len(trees)):\n",
    "        leafnotes = profile[\"leaves_per_tree\"][i]\n",
    "\n",
    "        nsubj_idxs = [i for i in leafnotes if leafnotes[i][\"token\"] == \"*pro*\" and \"info\" in leafnotes[i]]\n",
    "\n",
    "        for nsubj_idx in nsubj_idxs:\n",
    "            \n",
    "            #print(mentions)\n",
    "            n_subj_doc_level_pos = nsubj_idx + nsubj_counter\n",
    "            candidate_idxs = [idx for idx in mentions if idx < n_subj_doc_level_pos]\n",
    "            hyp_rankings = coref_map(candidate_idxs, profile['leaves'])\n",
    "            try:\n",
    "                verb_idx = get_verb_idx(trees[i], nsubj_idx) + nsubj_counter\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if verb_idx > context_len:\n",
    "                break\n",
    "            \n",
    "            for map_idx in range(len(attention_maps)):\n",
    "                #print(verb_idx)\n",
    "                candidate_attentions = [(candidate_idx, attention_maps[map_idx][verb_idx][candidate_idx]) for candidate_idx in candidate_idxs]\n",
    "                corefs = coref_map([tup[0] for tup in candidate_attentions], profile['leaves'])\n",
    "                attn_ranking = aggregate_attentions(candidate_attentions, corefs)\n",
    "                comparison = compare_rankings(hyp_rankings, attn_ranking)\n",
    "                if map_idx in comparisons:\n",
    "                    comparisons[map_idx].append(comparison)\n",
    "                else:\n",
    "                    comparisons[map_idx] = [comparison]\n",
    "\n",
    "        nsubj_counter += len(leafnotes)\n",
    "                \n",
    "    return comparisons\n",
    "         \n",
    "def test_hyp1(profile, attention_maps, doc_level=False, context_len=512):\n",
    "    \n",
    "    if doc_level:\n",
    "         print(\"whole doc\")\n",
    "         return test_hyp1_doc(profile, attention_maps, context_len)\n",
    "    else: \n",
    "         tree = Tree.fromstring(profile[\"tree\"])\n",
    "         all_mentions = mention_idxs([tree])\n",
    "\n",
    "    comparisons = {}\n",
    "    leafnotes = profile[\"leafnotes\"]\n",
    "    nsubj_idxs = [i for i in leafnotes if leafnotes[i][\"token\"] == \"*pro*\" and \"info\" in leafnotes[i]]\n",
    "    for nsubj_idx in nsubj_idxs:\n",
    "        verb_idx = get_verb_idx(tree, nsubj_idx)\n",
    "        candidate_idxs = [idx for idx in all_mentions if idx < nsubj_idx]\n",
    "        hyp_rankings = coref_map(candidate_idxs, leafnotes)\n",
    "        if verb_idx >= context_len:\n",
    "            break\n",
    "        for map_idx in range(len(attention_maps)):\n",
    "            candidate_attentions = [(candidate_idx, attention_maps[map_idx][verb_idx][candidate_idx]) for candidate_idx in candidate_idxs]\n",
    "            corefs = coref_map([tup[0] for tup in candidate_attentions], profile['leaves'])\n",
    "            attn_ranking = aggregate_attentions(candidate_attentions, corefs)\n",
    "            comparison = compare_rankings(hyp_rankings, attn_ranking)\n",
    "            if map_idx in comparisons:\n",
    "                comparisons[map_idx].append(comparison)\n",
    "            else:\n",
    "                comparisons[map_idx] = [comparison]\n",
    "\n",
    "    return comparisons\n",
    "            \n",
    "#to-do: character-level implementation\n",
    "\n",
    "#Hypothesis 2: grammatical role (ranking by grammatical role)\n",
    "#to-do: finish implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a894f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compares attention rankings to hypothesized rankings\n",
    "'''def test_hyp_1(profile, attention_maps, is_bidirectional = True):\n",
    "    tree = Tree.fromstring(profile[\"tree\"])\n",
    "    leafnotes = profile[\"leafnotes\"]\n",
    "    candidate_idxs = mention_idxs(tree)\n",
    "    nsubj_idxs = [i for i in leafnotes if leafnotes[i][\"token\"] == \"*pro*\" and \"info\" in leafnotes[i]]\n",
    "    for nsubj_idx in nsubj_idxs:\n",
    "        verb_idx = get_verb_idx(tree, nsubj_idx)\n",
    "        candidate_idxs = [idx for idx in candidate_idxs if idx < nsubj_idx]\n",
    "        for map in attention_maps:\n",
    "            candidate_attentions = [map[verb_idx][candidate] for candidate in candidate_idxs]\n",
    "            \n",
    "            if is_bidirectional:\n",
    "                candidate_attentions_reverse = [map[candidate][verb_idx] for candidate in candidate_idxs]'''\n",
    "    #to-do: finish implementation\n",
    "    \n",
    "#hypotheses = {1: test_hyp_1}      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "ff61c696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 32, 17, 17])\n",
      "(17, 17)\n",
      "17 ['▁就', '▁觉', '▁挺', '▁不', '▁舒服', '▁的', '▁', ',', '▁本来', '▁就是', '▁信心', '▁挺', '足', '▁的', '▁.', '<sep>', '<cls>']\n",
      "▁就 就\n",
      "▁觉 觉\n",
      "▁挺 挺\n",
      "▁不 不\n",
      "▁舒服 舒服\n",
      "▁的 的\n",
      ", ▁\n",
      "▁本来 ▁\n",
      "▁就是 ▁\n",
      "▁信心 ,\n",
      "▁挺 本来\n",
      "足 就是\n",
      "▁的 信心\n",
      "▁. 挺\n",
      "<sep> 足\n",
      "<cls> 的\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#viewer\n",
    "attention_matrix = attention_map(model, sample, heads, layers)\n",
    "figure = sns.heatmap(attention_matrix, xticklabels=2, yticklabels=2)\n",
    "figure.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "12825142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0, 0]}\n"
     ]
    }
   ],
   "source": [
    "#print(test_hyp1(doc_profile, [attention_matrix], doc_level=True, context_len = 512))\n",
    "print(test_hyp1(sample, [attention_matrix], doc_level=False, context_len = 512))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
