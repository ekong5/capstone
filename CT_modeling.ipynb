{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b1bd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Emma/Desktop/capstone/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.tree import Tree\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eaa017",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"ch_copy/00/\"\n",
    "doc_id = '0037'\n",
    "\n",
    "#returns dictionary containing plain sentence, constituency parse and coreference-id-labeled tokens \n",
    "def get_sentence_profiles(doc_id):\n",
    "    profiles = {}\n",
    "    filename = \"ch_\" + doc_id + \".onf\"\n",
    "    fp = dir + filename\n",
    "    with open(fp, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    #enumerated = enumerate(lines) -- don't do this\n",
    "    plain_sent_idxs = [i for i, txt in enumerate(lines) if txt == (\"Plain sentence:\\n\")]\n",
    "    treebanked_idxs = [i for i, txt in enumerate(lines) if txt.find(\"Treebanked sentence:\\n\") > -1]\n",
    "    tree_idxs = [i for i, txt in enumerate(lines) if txt == (\"Tree:\\n\")]\n",
    "    leaves_idxs = [i for i, txt in enumerate(lines) if txt == (\"Leaves:\\n\")]\n",
    "\n",
    "    n_sents = len(plain_sent_idxs)\n",
    "\n",
    "    for i in range(n_sents):\n",
    "        profile = doc_id + \"_\" + str(i)\n",
    "        profiles[profile] = {}\n",
    "        profiles[profile][\"plain\"] = lines[(plain_sent_idxs[i] + 2):treebanked_idxs[i]][0].strip()\n",
    "        profiles[profile][\"plain\"] = profiles[profile][\"plain\"].replace(\"--\", \"\")\n",
    "        raw_tree = lines[(tree_idxs[i]+2):leaves_idxs[i]]\n",
    "        profiles[profile][\"tree\"] = process_tree(raw_tree)\n",
    "\n",
    "        if i < n_sents - 1:\n",
    "            profiles[profile][\"leaves\"] = lines[(leaves_idxs[i]+2):(plain_sent_idxs[i+1]-3)]\n",
    "        else:\n",
    "            profiles[profile][\"leaves\"] = lines[(leaves_idxs[i]+2):-3]\n",
    "\n",
    "        profiles[profile][\"leaves\"] = process_leaves(profiles[profile][\"leaves\"])\n",
    "\n",
    "        i += 1\n",
    "    return profiles\n",
    "\n",
    "def process_leaves(leafnotes):\n",
    "    leaves_dict = {}\n",
    "    i = 0\n",
    "    for line in leafnotes:\n",
    "        line = line.strip()\n",
    "        if len(line) > 0:\n",
    "            line = line.split()\n",
    "            if line[0].isdigit():\n",
    "                if line[1] != \"--\":\n",
    "                    leaves_dict[i] = {\"token\":\"\", \"info\": {}}\n",
    "                    leaves_dict[i][\"token\"] = line[1]\n",
    "                    i += 1\n",
    "            elif i-1 in leaves_dict:\n",
    "                leaves_dict[i-1][\"info\"][line[0]] = line[1:]\n",
    "       \n",
    "    return leaves_dict\n",
    "\n",
    "def process_tree(raw_tree):\n",
    "    tree = \"\"\n",
    "    for line in raw_tree:\n",
    "        tree += line.strip()\n",
    "    return tree\n",
    "\n",
    "def get_doc_profile(doc_id):\n",
    "    profile = {}\n",
    "    sentence_profiles = get_sentence_profiles(doc_id)\n",
    "    profile[\"plain\"] = \"\"\n",
    "    for id in sentence_profiles:\n",
    "        profile[\"plain\"] += sentence_profiles[id][\"plain\"]\n",
    "    profile[\"tree\"] = [sentence_profiles[profile][\"tree\"] for profile in sentence_profiles]\n",
    "    profile[\"leaves\"] = sentence_profiles[doc_id + \"_0\"][\"leaves\"]\n",
    "    sentence_profiles_tail = dict(list(sentence_profiles.items())[1:])\n",
    "    i = list(profile[\"leaves\"].keys())[-1]\n",
    "    for id in sentence_profiles_tail:\n",
    "        for leaf in sentence_profiles[id]['leaves']:\n",
    "            i += 1\n",
    "            profile[\"leaves\"][i] = sentence_profiles_tail[id]['leaves'][leaf]\n",
    "    return profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "986e8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_profiles = get_sentence_profiles(doc_id)\n",
    "doc_profile = get_doc_profile(doc_id)\n",
    "sample = s_profiles['0037_0']#['plain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "448cfe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'token': '*pro*', 'info': {'coref:': ['IDENT', '4', '0-0', '*pro*']}}, 1: {'token': '就', 'info': {}}, 2: {'token': '觉', 'info': {'prop:': ['觉.01'], 'v': ['*', '->', '2:0,', '觉'], 'ARG0': ['*', '->', '0:0,', '*pro*'], 'ARGM-DIS': ['*', '->', '1:1,', '就'], 'ARG1': ['*', '->', '3:3,', '*pro*', '挺', '不', '舒服', '的']}}, 3: {'token': '*pro*', 'info': {'coref:': ['IDENT', '5', '3-3', '*pro*']}}, 4: {'token': '挺', 'info': {}}, 5: {'token': '不', 'info': {}}, 6: {'token': '舒服', 'info': {'prop:': ['舒服.02'], 'v': ['*', '->', '6:0,', '舒服'], 'ARG0': ['*', '->', '3:0,', '*pro*'], 'ARGM-ADV': ['*', '->', '5:1,', '不']}}, 7: {'token': '的', 'info': {}}, 8: {'token': ',', 'info': {}}, 9: {'token': '本来', 'info': {}}, 10: {'token': '就是', 'info': {}}, 11: {'token': '信心', 'info': {}}, 12: {'token': '挺', 'info': {}}, 13: {'token': '足', 'info': {'prop:': ['足.01'], 'v': ['*', '->', '13:0,', '足'], 'ARGM-ADV': ['*', '->', '12:1,', '挺'], 'ARGM-DIS': ['*', '->', '10:1,', '就是'], 'ARG0': ['*', '->', '11:1,', '信心']}}, 14: {'token': '的', 'info': {}}, 15: {'token': '.', 'info': {}}}\n"
     ]
    }
   ],
   "source": [
    "print(sample['leaves'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b2d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "\n",
    "##word-level\n",
    "def get_words(sentence_profile):\n",
    "    leaves = sentence_profile[\"leaves\"]\n",
    "    #words = [(idx, leaves[i][\"token\"]) for idx, i in enumerate(leaves)]\n",
    "    words = [leaves[i][\"token\"] for i in leaves]\n",
    "    return words\n",
    "\n",
    "##character-level\n",
    "def get_characters(sentence_profile):\n",
    "    #characters = [(idx, ch) for idx, ch in enumerate(sentence_profile[\"plain\"])]\n",
    "    characters = [ch for ch in sentence_profile[\"plain\"]]\n",
    "    return characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model and tokenizer\n",
    "model_name = \"ckiplab/gpt2-base-chinese\" #\"hfl/chinese-bert-wwm\" #\"bert-base-chinese\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77405ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = 0\n",
    "layers = 0\n",
    "\n",
    "#returns attention weights for specified head, layer\n",
    "def attention_map(model, text, heads, layers):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    print(torch.stack(outputs.attentions).shape)\n",
    "    attention_matrix = torch.stack(outputs.attentions)[layers, :, heads, :, :].squeeze(0).detach().numpy()\n",
    "    print(attention_matrix.shape)\n",
    "    # = attention[layers.unsqueeze(0), :, heads, :, :]\n",
    "    #print(selection.shape)\n",
    "    #aggregate = attention.sum(dim = 0).detach().numpy()\n",
    "    #print(aggregate.shape)\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    punct_idxs = [idx for idx, token in enumerate(tokens) if token in [\",\", \"[CLS]\", \"[SEP]\"]]\n",
    "    tokens = [token for i, token in enumerate(tokens) if i not in punct_idxs]\n",
    "    attention_matrix = np.delete(attention_matrix, punct_idxs, axis = 1)\n",
    "    attention_matrix = np.delete(attention_matrix, punct_idxs, axis = 0)\n",
    "    \n",
    "    return attention_matrix #return aggregate.detach().numpy()\n",
    "    #tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6330eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the index of some verb in the predicate corresponding to the null subject\n",
    "def get_verb_idx(tree, nsubj_idx):\n",
    "    t_positions = tree.treepositions(order = 'preorder')\n",
    "    leaf_positions = [tree.leaf_treeposition(idx) for idx in range(len(tree.leaves()))]\n",
    "    nsubj_leaf_position = tree.leaf_treeposition(nsubj_idx)\n",
    "    IP_idx = t_positions.index(nsubj_leaf_position) - 3\n",
    "    IP_pos = t_positions[IP_idx]\n",
    "    IP_tree = tree[IP_pos]\n",
    "    IPt_positions = IP_tree.treepositions(order='preorder')\n",
    "    mv_idx = None\n",
    "    for i in range(len(IPt_positions)):\n",
    "        position = IPt_positions[i]\n",
    "        if type(IP_tree[position]) == str and IP_tree[IPt_positions[i-1]].label() == 'VV':\n",
    "            print(IP_tree[IPt_positions[i-1]])\n",
    "            mv_idx = leaf_positions.index(t_positions[IP_idx + i])\n",
    "            print(mv_idx)\n",
    "            return mv_idx\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110369bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for layers adhering to Cf rankings\n",
    "\n",
    "#returns ordered indices of candidate antecedents\n",
    "'''def mention_sequence(tree):\n",
    "    lleaves = tree.pos()\n",
    "    return [(leaf[0], i) for i, leaf in enumerate(lleaves) if \"N\" in leaf[1] and \"*-\" not in leaf[0]]'''\n",
    "def mention_idxs(tree):\n",
    "    return [i for i, leaf in enumerate(tree.pos()) if \"N\" in leaf[1] and \"*-\" not in leaf[0]]\n",
    "'''def mention_set(string):\n",
    "    return set(mention_sequence(string)) #ensure that order is preserved'''\n",
    "\n",
    "#Hypothesis 1：linear order (looking for first-mention effect)\n",
    "def coref_map(tree, leafnotes): #map candidate antecedents to coreference ids\n",
    "    candidate_idxs = mention_idxs(tree)\n",
    "    map = {}\n",
    "    candidate_leafnotes = [leafnotes[idx] for idx in candidate_idxs]\n",
    "    for idx in leafnotes:\n",
    "        map[idx] = {idx: None}\n",
    "        if leafnotes[idx][\"info\"] and leafnotes[idx][\"info\"][\"coref\"]:\n",
    "                map[idx][idx] = leafnotes[idx][\"info\"][\"coref\"][1]\n",
    "                #...to-do: finish implementation\n",
    "                \n",
    "def test_hyp_1(profile, attention_maps, is_bidirectional = True):\n",
    "    tree = Tree.fromstring(profile[\"tree\"])\n",
    "    leafnotes = profile[\"leaves\"]\n",
    "    candidate_idxs = mention_idxs(tree)\n",
    "    nsubj_idxs = [i for i in leafnotes if leafnotes[i][\"token\"] == \"*pro*\" and leafnotes[\"info\"]]\n",
    "    for nsubj_idx in nsubj_idxs:\n",
    "        verb_idx = get_verb_idx(tree, nsubj_idx)\n",
    "        candidate_idxs = [idx for idx in candidate_idxs if idx < nsubj_idx]\n",
    "        for map in attention_maps:\n",
    "            candidate_attentions = [map[verb_idx][candidate] for candidate in candidate_idxs]\n",
    "            \n",
    "            '''if is_bidirectional:\n",
    "                candidate_attentions_reverse = [map[candidate][verb_idx] for candidate in candidate_idxs]'''\n",
    "            #...to-do: finish implementation\n",
    "            \n",
    "#to-do: character-level implementation\n",
    "\n",
    "#Hypothesis 2: grammatical role (ranking by grammatical role)\n",
    "#to-do: finish implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a894f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compares attention rankings to hypothesized rankings\n",
    "def test_hyp_1(profile, attention_maps, is_bidirectional = True):\n",
    "    tree = Tree.fromstring(profile[\"tree\"])\n",
    "    leafnotes = profile[\"leaves\"]\n",
    "    candidate_idxs = mention_idxs(tree)\n",
    "    nsubj_idxs = [i for i in leafnotes if leafnotes[i][\"token\"] == \"*pro*\" and leafnotes[\"info\"]]\n",
    "    for nsubj_idx in nsubj_idxs:\n",
    "        verb_idx = get_verb_idx(tree, nsubj_idx)\n",
    "        candidate_idxs = [idx for idx in candidate_idxs if idx < nsubj_idx]\n",
    "        for map in attention_maps:\n",
    "            candidate_attentions = [map[verb_idx][candidate] for candidate in candidate_idxs]\n",
    "            \n",
    "            '''if is_bidirectional:\n",
    "                candidate_attentions_reverse = [map[candidate][verb_idx] for candidate in candidate_idxs]'''\n",
    "    #to-do: finish implementation\n",
    "    \n",
    "hypotheses = {1: test_hyp_1}      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewer\n",
    "attention_matrix = attention_map(model, sample['plain'], heads, layers)\n",
    "figure = sns.heatmap(attention_matrix, xticklabels=2, yticklabels=2)\n",
    "figure.invert_yaxis()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
